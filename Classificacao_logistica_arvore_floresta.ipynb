{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a055d775",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbaf417",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b7690",
   "metadata": {},
   "source": [
    "É uma técnica recomendada para situações em que a variável dependente é de natureza dicotômica ou binária. Quanto às independentes, tanto podem ser categóricas ou não.\n",
    "\n",
    "A regressão logística é um recurso que nos permite **estimar a probabilidade** associada à ocorrência de determinado evento em face de um conjunto de variáveis explanatórias.\n",
    "\n",
    "No caso da variável dependente Y assumir apenas dois possíveis estados (1 ou 0) e haver um conjunto de p variáveis independentes $x_1 , x_2, ... , x_n$, o modelo de regressão logística pode ser escrito da seguinte forma:\n",
    "\n",
    "$$\n",
    "Y = \\frac{1}{1+e^{-g(x)}}\n",
    "$$\n",
    "$$\n",
    "g(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\: +\\:...\\:+\\: \\beta_nx_n\\:\\:\\:\\:\\: (equacao\\: da\\: reta)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Os coeficientes $\\beta_0, \\beta_1, \\:...\\:, \\beta_n $ são estimados a partir do conjunto dados, pelo método da **máxima verossimilhança**, em que encontra uma combinação de coeficientes que maximiza a probabilidade da amostra ter sido observada.\n",
    "\n",
    "<img src='imagens_classificacao/regressao_logistica.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e9e20",
   "metadata": {},
   "source": [
    "## Árvore de Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40934b",
   "metadata": {},
   "source": [
    "Em uma árvore de classificação, cada nó interno corresponde a um teste em um atributo, cada ramificação representa a saída de um teste e cada nó folha representa um rótudo de classe.\n",
    "\n",
    "- é construida de maneira top-down\n",
    "- testa-se atributo por atributo para ver a melhor separação dos targets\n",
    "\n",
    "- gera muito overfitting caso a arvore seja muito profunda, com isso, a ideia é limitar\n",
    "\n",
    "\n",
    "<img src='imagens_classificacao/arvore1.png'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Para a clissificação é utilizada o critério de impureza, em sua maioria via **entropia**, ou seja supondo que para um nó com $p$ exemplos positivos e $n$ exemplos negativos, temos que:\n",
    "\n",
    "$$\n",
    "entropia = -[(\\frac{p}{p+n}\\log_2\\frac{p}{p+n})+(\\frac{n}{p+n}\\log_2\\frac{n}{p+n})]\n",
    "$$\n",
    "\n",
    "ou seja, **para duas classes**, probabilidade de $p\\log_2p$ + proabilidade de $n\\log_2n$ \n",
    "\n",
    "\n",
    "Para calculo via Gini, a equação é dadda por\n",
    "\n",
    "$$\n",
    "gini = 1 - \\sum_{i = 1}^{classes}p_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "E, por sua vez, o **ganho de informação (GI)** é dado pela impureza do target menos a impureza da feature\n",
    "\n",
    "<img src='imagens_classificacao/entropia.png'>\n",
    "\n",
    "\n",
    "<img src='imagens_classificacao/arvore2.png'>\n",
    "\n",
    "\n",
    "**Ou seja, se tivermos 50% dos casos pertencentes a classe 1 e 50% a classe 2, a entropia irá dar 1, o que quer dizer que é pouco divisível. Se tivermos 100% dos casos para a classe 1, a entropia irá dar 0, o que é muito bem divisível**. Ou seja, serve para a árvore pegar as features que possuem maior divisibilidade (mais importantes).\n",
    "\n",
    "- para diminuir o overfitting, pode-se realizar o procedimento de poda, eliminando profundidade do árvore\n",
    "\n",
    "Ele funciona como se tivesse recortando o gráfico a partir das saídas, ou seja, árvores mais profundas geram recortes mais precisos, porém, mais overfitting\n",
    "\n",
    "Ex:\n",
    "\n",
    "<img src='imagens_classificacao/arvore3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1a3a7",
   "metadata": {},
   "source": [
    "## Floresta Aleatória"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95127900",
   "metadata": {},
   "source": [
    "A floresta aleatória (Random Forest) é basicamente a junção de várias àvores de decisão aleatórias com o intuito de diminuir o overfitting, no qual é passado para o algorítimo a quantidade de árvores desejada\n",
    "\n",
    "Basicamente separa-se o dataset em varios datasets aleatórios e cada dataset gra uma árvore na floresta.\n",
    "\n",
    "No classificador.predict, a entrada é passada por todas as árvores e a saída da floresta é a classe que mais aparece nas saídas das árvores (moda), **a que recebe mais votos**.\n",
    "\n",
    "\n",
    "<img src='imagens_classificacao/random-forest.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
